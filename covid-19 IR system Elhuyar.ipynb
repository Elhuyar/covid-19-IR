{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Wide perspective query system focused on COVID-19\n**\n\nSystem that offers integral consultation of scientific papers on COVID-19 through search at document and passage level and with auxiliary visualization of the results.\n\nThe system has the following features:\n* Simultaneous and complementary retrieval of documents (coarse grain) and passages (fine grain) relevant to queries.\n* Visual representation of relevant documents and paragraphs according to their semantic content.\n* Hybrid retrieval of paragraphs and answers.\n\nTechniques:\n* Recovery of documents through language models (Indri).\n* Recovery of passages by combining language models (Indri) and re-ranking based on fine-tuned BERT.\n* Visualization by means of embeddings and reduction of dimensions.\n\nContributions:\n* Results and visualization according to different techniques that offer an enriched and wide perspective consultation.\n* Fine-tuning by trainset built from titles and abstracts.\n"},{"metadata":{},"cell_type":"markdown","source":"\n**Preprocess of collections\n**\nFiltering by keywords and json containing paragraphs.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport sys\nimport re\nimport collections\n\nimport csv\n\n# Generate regex patterns for filter words, for the moment we use a simple list of terms, this ought to be more sophisticated                                                                                                                                                                                     \nfilterwords=[\"2019-nCoV\",\"COVID-19\",\"novel coronavirus\",\"SARS-CoV-2\",\"Wuhan coronavirus\"]\nword_patterns=collections.OrderedDict()\nfor w in filterwords:\n    wrdlc=w.lower()\n    wrdptrn=re.compile(r\"\\b\"+re.escape(wrdlc))\n    word_patterns[wrdlc]=wrdptrn\n\n    \noutput=[]\nprocessed={}\nmetadata=csv.DictReader('kaggle/input/metadata.csv', dialect='excel')#delimiter='\\t',  quoting=csv.QUOTE_NONE) #.drop_duplicates()                                                                                                  \n\nof=open(out_file,\"w\", encoding='utf-8')\nfieldnames=metadata.fieldnames\nfieldnames.append('keywords_found')\nsys.stderr.write(\"fields: {}\\n\".format(fieldnames))\n\nwr=csv.DictWriter(of,fieldnames=fieldnames, dialect='excel')\nwr.writeheader()\n\nskipped=0\ndocs_found=0\nfile_problems=0\nfor row in metadata:\n    proces_count+=1\n        #if proces_count > 10:                                                                                                                                                                                     \n        #    sys.exit(100)                                                                                                                                                                                         \n        sys.stderr.write(\"\\r {a:8d} documents processed\".format(a=proces_count))\n        #sys.stderr.write(\"\\n document sha {} and pmcid {} --> \\n row {}\\n\".format(row[\"sha\"],row[\"pmcid\"],row))                                                                                                   \n        #we give preference to sha over pmc                                                                                                                                                                        \n        file_id=row[\"sha\"]\n        file_type=\"pdf_json\"\n        if row[\"sha\"] == None or row[\"sha\"] == '':\n            file_id=row[\"pmcid\"]\n            file_type=\"pmc_json\"\n            #sys.stderr.write(\"WARN: document {} has no sha {}\\n\".format(row[\"cord_uid\"],row[\"sha\"]))                                                                                                              \n\n            if row[\"pmcid\"] == None or row[\"pmcid\"] == '':\n                skipped+=1\n                sys.stderr.write(\"WARN: document {} has neither sha nor pmcid, skipping ({})\\n\".format(row[\"cord_uid\"],skipped))\n                continue\n\n        if file_id in processed:\n            sys.stderr.write(\"WARN: document with file_id {} (sha or pmcid) already processed, skipping\\n\".format(file_id))\n        else:\n            processed[file_id]=1\n            if row[\"sha\"] != None and row[\"pmcid\"] != None:\n                processed[row[\"pmcid\"]]=1\n\n        extension=\".json\"\n    \n    \n    \n    \n    \n    \n# Walk over the collection and find relevant documents, +\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Document collection in TREC format.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Paragraph/passage collection in TREC format.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nIndexing document collection\nIndexing paragraph/passage collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Reranking by means of Fine-tuned BERT for sentence pair classification\n**\n\nClinical BERT (Bio+ClinicalBERT) (Alsentzer et al., 2019) finetuned for sentence pair classification, over two datasets:\n- Titles as questions + abstracts as answers, from CORD-19 kaggle dataset. \n- MedQuAD question answering dataset (Asma {Ben Abacha} and Dina Demner{-}Fushman, 2019)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"The first step is preparing the dataset for finetuning BERT models. The title-abstract collection is straightforward, we just need to extract title and abstract pairs in BERT readable format. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import csv\nimport random\n\n\ntitles={}\nabsts={}\n\n\nwith open('/data/input/metadata.csv') as tsvfile:    \n    reader = csv.reader(tsvfile, delimiter=',',quotechar='\"')\n    #Don't print header\n    next(reader)\n    for row in reader:\n        dokid=row[0]\n        title=row[3]\n        abstract=row[8]\n        if abstract.strip() != \"\" and title.strip() != \"\": \n            titles[dokid]=title\n            absts[dokid]=abstract\n\n\nfor dokid in titles:\n    # positive examples\n    print(titles[dokid]+\"\\t\"+dokid+\"\\t\"+absts[dokid]+\"\\t\"+dokid+\"\\t1\")\n    #negative examples (1:10 positve:negative ratio)\n    for i in range(10):\n        dokid2=random.choice(list(absts.keys()))\n        print(titles[dokid]+\"\\t\"+dokid+\"\\t\"+absts[dokid2]+\"\\t\"+dokid2+\"\\t0\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for the finetuning part we use original BERT distribution [run_classifier.py](https://github.com/google-research/bert/blob/master/run_classifier.py) finetuning script, with a custom data processor very similar to the one used for MRPC dataset (only minimal changes done to the _create_examples function to adapt it to our needs.). "},{"metadata":{"trusted":true},"cell_type":"code","source":"class CovidKaggleProcessor(DataProcessor):\n  \"\"\"...\"\"\"\n\n  def get_labels(self):\n    \"\"\"See base class.\"\"\"\n    return [\"0\", \"1\"]\n\n  def _create_examples(self, lines, set_type):\n    \"\"\"Creates examples for the training and dev sets.\"\"\"\n    examples = []\n    for (i, line) in enumerate(lines):\n      guid = \"%s-%s\" % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[2])\n      if set_type == \"test\":\n        label = tokenization.convert_to_unicode(line[4])\n    else:\n        label = tokenization.convert_to_unicode(line[4])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finetuning is done over Bio+ClinicalBERT for 4 epochs, with a bastch size of 16 in order to fit into our GPU (GeForce 2080 RTX Ti). Following the exact command used:"},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_BASE_DIR=\"clinicalBert/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000\"\n\npython -u run_classifier.py  --task_name=covid \\\n       --do_train=true \\\n       --do_eval=true \\\n       --data_dir=$GLUE_DIR \\\n       --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n       --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n       --init_checkpoint=$BERT_BASE_DIR/model.ckpt-150000 \\\n       --max_seq_length=128 \\\n       --train_batch_size=16 \\\n       --learning_rate=2e-5 \\\n       --num_train_epochs=4.0 \\\n       --output_dir=$GLUE_DIR/output-4e-1 \\\n       --do_lower_case=False \\","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training results for reranking finetuning:\n\n\n------------ Result of Bert fine-tuned model ----------\n\n              precision    recall  f1-score   support\n\n           0     0.9962    0.9976    0.9969     77791\n           1     0.9761    0.9620    0.9690      7782\n\n    accuracy                         0.9944     85573\n   macro avg     0.9862    0.9798    0.9830     85573\nweighted avg     0.9944    0.9944    0.9944     85573\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}